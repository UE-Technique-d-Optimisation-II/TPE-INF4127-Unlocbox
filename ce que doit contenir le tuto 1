Voici une description détaillée en français du contenu du tutoriel LaTeX fourni, qui est une présentation Beamer sur l'**optimisation et la visualisation de réseaux de neurones avec UNLocBoX**, une boîte à outils MATLAB pour l'optimisation convexe. Le tutoriel se concentre sur l'entraînement d'un réseau de neurones sur le jeu de données MNIST avec des régularisations L1 et L2, en mettant l'accent sur la sparsité et l'efficacité computationnelle. La présentation est structurée en 7 sections, avec une annexe contenant des fonctions auxiliaires. Chaque section est décrite ci-dessous, avec un résumé des concepts, du code MATLAB, des équations mathématiques et des visualisations.

---

### Structure du Document
- **Classe du document** : Beamer avec un rapport d'aspect 16:9 (`\documentclass[aspectratio=169]{beamer}`).
- **Thème** : Thème Madrid avec une palette de couleurs personnalisée (bleu primaire, bleu secondaire, orange accentué, gris clair, gris foncé).
- **Packages utilisés** :
  - `inputenc`, `fontenc`, `babel` (français) pour l'encodage et le support linguistique.
  - `amsmath`, `amssymb`, `amsthm` pour les formules mathématiques.
  - `graphicx` pour les images (logo commenté).
  - `listings` pour les extraits de code MATLAB, stylisés avec `matlabstyle`.
  - `xcolor` pour les couleurs personnalisées.
  - `tikz` pour les diagrammes (réseaux de neurones, matrice de confusion, etc.).
  - `algorithm`, `algpseudocode` pour le pseudocode (algorithme FISTA).
  - `hyperref` pour les URLs cliquables.
- **Configuration des listings** : Le code MATLAB est affiché avec le style `matlabstyle`, incluant la coloration syntaxique, les numéros de ligne, et un fond gris clair.
- **Métadonnées** : Titre, sous-titre, auteur, institution et date sont des placeholders (ex. : `Votre Nom`, `Votre Institution`).
- **Structure** : La présentation est divisée en 7 sections, suivies d'une annexe. Chaque section contient plusieurs diapositives (frames) avec du texte, des équations, du code et des visualisations.

---

### Contenu Détaillé par Section

#### Diapositive de Titre
- **Contenu** : Affiche le titre, le sous-titre, l'auteur, l'institution et la date.
- **Titre** : "Optimisation et Visualisation de Réseaux de Neurones".
- **Sous-titre** : "Application d'UNLocBox aux Fonctions Convexes".
- **Objectif** : Introduit le sujet et établit le contexte du tutoriel.

#### Table des Matières
- **Titre de la diapositive** : "Plan de la Présentation".
- **Contenu** : Liste les sections de la présentation :
  1. Introduction
  2. Configuration de l'Environnement
  3. Création du Réseau de Neurones
  4. Formulation du Problème d'Optimisation
  5. Optimisation avec UNLocBox
  6. Visualisation des Résultats
  7. Conclusion et Extensions
- **Objectif** : Fournit une vue d'ensemble du déroulement de la présentation.

---

### Section 1 : Introduction

#### Diapositive : Contexte et Motivation
- **Contenu** :
  - **Colonne gauche (Texte)** :
    - Décrit les réseaux de neurones artificiels comme des modèles inspirés du cerveau biologique.
    - Met en avant l'architecture multi-couches, l'apprentissage par rétropropagation et les applications (ex. : vision par ordinateur, traitement du langage naturel).
  - **Colonne droite (Diagramme)** :
    - Un diagramme TikZ représentant un réseau de neurones simple avec :
      - Couche d'entrée (3 nœuds).
      - Couche cachée (4 nœuds).
      - Couche de sortie (2 nœuds).
      - Connexions entre les couches avec des étiquettes ("Entrée", "Cachée", "Sortie").
- **Objectif** : Introduit les réseaux de neurones et leurs applications avec une visualisation.

#### Diapositive : Problématique de l'Optimisation
- **Contenu** :
  - **Défi principal** : L'entraînement d'un réseau de neurones nécessite la minimisation d'une fonction de coût :
    \[
    \mathcal{L}(\mathbf{W}, \mathbf{b}) = \frac{1}{N}\sum_{i=1}^{N} \ell(f(\mathbf{x}_i; \mathbf{W}, \mathbf{b}), y_i)
    \]
  - **Difficultés** :
    - Fonctions non convexes (minima locaux).
    - Haute dimensionnalité.
    - Sur-apprentissage (overfitting).
    - Coût computationnel élevé.
- **Objectif** : Présente les défis liés à l'optimisation des réseaux de neurones.

#### Diapositive : Pourquoi UNLocBox ?
- **Contenu** :
  - **Colonne gauche (Texte)** :
    - Présente UNLocBox comme une boîte à outils MATLAB pour l'optimisation, spécialisée dans les problèmes non lisses.
    - Liste les avantages pour les réseaux de neurones : régularisation L1 (sparsité), convergence garantie, flexibilité des contraintes.
  - **Colonne droite (Bloc)** :
    - Formulation standard pour UNLocBox :
      \[
      \min_{\mathbf{x}} f_1(\mathbf{x}) + f_2(\mathbf{x})
      \]
      où \( f_1 \) est une fonction lisse et \( f_2 \) est une fonction proximale.
- **Objectif** : Justifie l'utilisation d'UNLocBox pour l'optimisation des réseaux de neurones.

---

### Section 2 : Configuration de l'Environnement

#### Diapositive : Installation d'UNLocBox
- **Contenu** :
  - **Étapes** :
    1. Télécharger UNLocBox depuis l'URL fournie.
    2. Extraire l'archive dans un dossier MATLAB.
    3. Ajouter au chemin MATLAB.
  - **Code** :
    ```matlab
    % Configuration du path
    addpath(genpath('chemin/vers/unlocbox'));

    % Verification de l'installation
    unlocbox_info

    % Test rapide
    x = randn(100,1);
    f.eval = @(x) norm(x)^2;
    f.grad = @(x) 2*x;
    param.verbose = 1;
    [sol, info] = forward_backward(x, f, param);
    ```
- **Objectif** : Guide l'audience sur l'installation et la vérification d'UNLocBox.

#### Diapositive : Dépendances et Préparation
- **Contenu** :
  - **Boîtes à outils MATLAB requises** :
    - Deep Learning Toolbox (optionnelle).
    - Statistics and Machine Learning Toolbox.
    - Image Processing Toolbox (pour la visualisation).
  - **Code** :
    ```matlab
    % Verification des toolboxes
    ver

    % Chargement des donnees MNIST
    load('mnist_data.mat'); % ou utiliser mnist_data()

    % Normalisation
    X_train = double(X_train) / 255;
    X_test = double(X_test) / 255;

    % Conversion des labels en one-hot encoding
    Y_train = ind2vec(y_train')';
    Y_test = ind2vec(y_test')';
    ```
- **Objectif** : Spécifie les dépendances et montre le prétraitement des données MNIST.

---

### Section 3 : Création du Réseau de Neurones

#### Diapositive : Architecture Proposée
- **Contenu** :
  - **Diagramme** : Visualisation TikZ d'un réseau de neurones avec :
    - Couche d'entrée : 784 dimensions (images MNIST 28×28).
    - Couche cachée 1 : 128 neurones (ReLU).
    - Couche cachée 2 : 64 neurones (ReLU).
    - Couche de sortie : 10 classes (Softmax).
    - Connexions étiquetées avec les matrices de poids (\(\mathbf{W}_1, \mathbf{W}_2, \mathbf{W}_3\)).
  - **Paramètres** :
    - Entrée : 784 dimensions.
    - Couches cachées : 128 et 64 neurones avec ReLU.
    - Sortie : 10 classes avec Softmax.
    - Total de paramètres : ~109 000.
- **Objectif** : Décrit l'architecture du réseau pour la classification MNIST.

#### Diapositive : Initialisation des Paramètres
- **Contenu** :
  - **Code** : Initialise le réseau avec l'initialisation Xavier/Glorot :
    ```matlab
    % Dimensions des couches
    layer_sizes = [784, 128, 64, 10];
    n_layers = length(layer_sizes) - 1;

    % Initialisation Xavier/Glorot
    for i = 1:n_layers
        n_in = layer_sizes(i);
        n_out = layer_sizes(i+1);
        
        % Poids
        limit = sqrt(6 / (n_in + n_out));
        W{i} = 2 * limit * rand(n_out, n_in) - limit;
        
        % Biais
        b{i} = zeros(n_out, 1);
    end

    % Concatenation en un vecteur (pour optimisation)
    theta = pack_params(W, b);
    ```
- **Objectif** : Montre comment initialiser les poids et biais du réseau.

#### Diapositive : Fonctions d'Activation
- **Contenu** :
  - **Colonne gauche (ReLU)** :
    - Équation : \(\text{ReLU}(x) = \max(0, x)\)
    - Code :
      ```matlab
      function a = relu(z)
          a = max(0, z);
      end

      function grad = relu_grad(z)
          grad = double(z > 0);
      end
      ```
  - **Colonne droite (Softmax)** :
    - Équation : \(\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}\)
    - Code :
      ```matlab
      function a = softmax(z)
          exp_z = exp(z - max(z));
          a = exp_z ./ sum(exp_z);
      end
      ```
- **Objectif** : Présente les fonctions d'activation ReLU et Softmax avec leur implémentation MATLAB.

---

### Section 4 : Formulation du Problème d'Optimisation

#### Diapositive : Fonction de Coût : Cross-Entropy
- **Contenu** :
  - **Bloc (Perte d'entropie croisée)** :
    - Pour la classification multi-classe :
      \[
      \mathcal{L}(\mathbf{W}, \mathbf{b}) = -\frac{1}{N}\sum_{i=1}^{N}\sum_{k=1}^{K} y_{ik} \log(\hat{y}_{ik})
      \]
      où :
      - \( N \) : nombre d'exemples.
      - \( K \) : nombre de classes (10 pour MNIST).
      - \( y_{ik} \) : label vrai (one-hot).
      - \( \hat{y}_{ik} \) : prédiction du réseau.
  - **Propriétés** :
    - Différentiable.
    - Pénalise fortement les mauvaises prédictions confiantes.
    - Convexe par rapport aux activations de la dernière couche.
- **Objectif** : Explique la fonction de coût utilisée pour l'entraînement.

#### Diapositive : Régularisation L1 et L2
- **Contenu** :
  - **Bloc (Elastic Net)** :
    - Terme de régularisation combiné :
      \[
      \mathcal{J}(\mathbf{W}, \mathbf{b}) = \mathcal{L}(\mathbf{W}, \mathbf{b}) + \lambda_1 \|\mathbf{W}\|_1 + \frac{\lambda_2}{2} \|\mathbf{W}\|_2^2
      \]
  - **Colonnes** :
    - **Régularisation L1 (\(\ell_1\))** :
      - Encourage la sparsité.
      - Sélection automatique de caractéristiques.
      - Non différentiable en 0.
      - \(\|\mathbf{W}\|_1 = \sum_{ij} |w_{ij}|\).
    - **Régularisation L2 (\(\ell_2\))** :
      - Pénalise les grands poids.
      - Différentiable partout.
      - Favorise des poids petits.
      - \(\|\mathbf{W}\|_2^2 = \sum_{ij} w_{ij}^2\).
- **Objectif** : Décrit les régularisations L1 et L2 et leurs effets.

#### Diapositive : Reformulation pour UNLocBox
- **Contenu** :
  - **Décomposition du problème** :
    \[
    \min_{\boldsymbol{\theta}} \quad \underbrace{f_1(\boldsymbol{\theta})}_{\text{fonction lisse}} + \underbrace{f_2(\boldsymbol{\theta})}_{\text{fonction proximale}}
    \]
    où :
    \[
    f_1(\boldsymbol{\theta}) = \mathcal{L}(\boldsymbol{\theta}) + \frac{\lambda_2}{2} \|\mathbf{W}\|_2^2 \quad \text{(lisse et différentiable)}
    \]
    \[
    f_2(\boldsymbol{\theta}) = \lambda_1 \|\mathbf{W}\|_1 \quad \text{(non-lisse)}
    \]
  - **Bloc (Opérateur proximal pour L1)** :
    - L'opérateur proximal pour \(\ell_1\) est le seuillage doux :
      \[
      \text{prox}_{\lambda_1 \|\cdot\|_1}(w) = \text{sign}(w) \cdot \max(|w| - \lambda_1, 0)
      \]
- **Objectif** : Montre comment reformuler le problème pour utiliser UNLocBox.

---

### Section 5 : Optimisation avec UNLocBox

#### Diapositive : Propagation Avant (Forward Pass)
- **Contenu** :
  - **Code** : Implémente la propagation avant dans le réseau :
    ```matlab
    function [y_pred, cache] = forward_pass(X, W, b)
        % Initialisation
        cache.a{1} = X;
        n_layers = length(W);
        
        % Propagation a travers les couches
        for i = 1:n_layers-1
            % Pre-activation
            cache.z{i+1} = W{i} * cache.a{i} + b{i};
            
            % Activation ReLU
            cache.a{i+1} = relu(cache.z{i+1});
        end
        
        % Derniere couche (Softmax)
        cache.z{n_layers+1} = W{n_layers} * cache.a{n_layers} + b{n_layers};
        y_pred = softmax(cache.z{n_layers+1});
        cache.a{n_layers+1} = y_pred;
    end
    ```
- **Objectif** : Montre comment calculer les prédictions du réseau.

#### Diapositive : Rétropropagation (Backward Pass)
- **Contenu** :
  - **Code** : Implémente la rétropropagation pour calculer les gradients :
    ```matlab
    function [grad_W, grad_b] = backward_pass(Y, cache, W)
        n_layers = length(W);
        N = size(Y, 2);
        
        % Gradient de la derniere couche
        delta{n_layers+1} = cache.a{n_layers+1} - Y;
        
        % Retropropagation
        for i = n_layers:-1:1
            % Gradients
            grad_W{i} = (1/N) * delta{i+1} * cache.a{i}';
            grad_b{i} = (1/N) * sum(delta{i+1}, 2);
            
            % Propagation du gradient
            if i > 1
                delta{i} = W{i}' * delta{i+1} .* relu_grad(cache.z{i});
            end
        end
    end
    ```
- **Objectif** : Explique le calcul des gradients pour l'optimisation.

#### Diapositive : Configuration des Fonctions UNLocBox
- **Contenu** :
  - **Code** : Définit les fonctions pour UNLocBox :
    ```matlab
    % Parametres
    lambda1 = 0.001; % Regularisation L1
    lambda2 = 0.01;  % Regularisation L2

    % Fonction f1: Perte + L2 (lisse)
    f1.eval = @(theta) compute_loss_L2(theta, X_train, Y_train, lambda2);
    f1.grad = @(theta) compute_gradient(theta, X_train, Y_train, lambda2);

    % Fonction f2: Regularisation L1 (proximale)
    f2.eval = @(theta) lambda1 * norm(theta(:), 1);
    f2.prox = @(theta, gamma) soft_threshold(theta, gamma * lambda1);

    % Fonction de seuillage doux
    function theta_prox = soft_threshold(theta, lambda)
        theta_prox = sign(theta) .* max(abs(theta) - lambda, 0);
    end
    ```
- **Objectif** : Configure les fonctions objectif et proximale pour l'optimisation.

#### Diapositive : Algorithme Forward-Backward (FISTA)
- **Contenu** :
  - **Pseudocode** : Algorithme FISTA (Forward-Backward Splitting) :
    ```latex
    \begin{algorithm}[H]
        \caption{Forward-Backward Splitting (FISTA)}
        \begin{algorithmic}[1]
            \State Initialiser $\boldsymbol{\theta}_0$, $t_0 = 1$
            \For{$k = 0, 1, 2, \ldots$}
                \State $\boldsymbol{\theta}_{k+1/2} = \boldsymbol{\theta}_k - \gamma \nabla f_1(\boldsymbol{\theta}_k)$ \Comment{Gradient step}
                \State $\boldsymbol{\theta}_{k+1} = \text{prox}_{\gamma f_2}(\boldsymbol{\theta}_{k+1/2})$ \Comment{Proximal step}
                \State $t_{k+1} = \frac{1 + \sqrt{1 + 4t_k^2}}{2}$ \Comment{Nesterov momentum}
                \State $\boldsymbol{\theta}_k = \boldsymbol{\theta}_{k+1} + \frac{t_k - 1}{t_{k+1}}(\boldsymbol{\theta}_{k+1} - \boldsymbol{\theta}_k)$
            \EndFor
        \end{algorithmic}
    \end{algorithm}
    ```
  - **Avantages** :
    - Convergence accélérée : \( O(1/k^2) \).
    - Adapté aux problèmes composites.
- **Objectif** : Présente l'algorithme FISTA utilisé par UNLocBox.

#### Diapositive : Exécution de l'Optimisation
- **Contenu** :
  - **Code** : Exécute l'optimisation avec FISTA :
    ```matlab
    % Parametres d'optimisation
    param.verbose = 1;        % Affichage
    param.maxit = 100;        % Iterations max
    param.tol = 1e-4;         % Tolerance
    param.gamma = 0.01;       % Pas de gradient

    % Initialisation
    theta_init = pack_params(W, b);

    % Optimisation avec Forward-Backward (FISTA)
    [theta_opt, info] = forward_backward(theta_init, f1, f2, param);

    % Extraction des parametres optimises
    [W_opt, b_opt] = unpack_params(theta_opt, layer_sizes);

    % Evaluation
    [y_pred, ~] = forward_pass(X_test, W_opt, b_opt);
    accuracy = mean(argmax(y_pred) == argmax(Y_test));
    fprintf('Accuracy: %.2f%%\n', accuracy * 100);
    ```
- **Objectif** : Montre comment lancer l'optimisation et évaluer les performances.

---

### Section 6 : Visualisation des Résultats

#### Diapositive : Courbes de Convergence
- **Contenu** :
  - **Diagramme** : Un graphique TikZ montrant la convergence de la fonction de coût :
    - Courbe bleue : avec régularisation L1 (\(\lambda_1 = 0.001\)).
    - Courbe orange : sans régularisation.
    - Échelle logarithmique sur l'axe des ordonnées.
  - **Observations** :
    - Convergence plus stable avec L1.
    - Légère augmentation du coût due au compromis sparsité/performance.
- **Objectif** : Visualise l'impact de la régularisation L1 sur la convergence.

#### Diapositive : Analyse de la Sparsité
- **Contenu** :
  - **Colonne gauche (Distribution des poids)** :
    - Code pour afficher les histogrammes des poids avant et après optimisation :
      ```matlab
      % Histogramme des poids
      figure;
      subplot(1,2,1);
      histogram(W_init(:), 50);
      title('Avant optimisation');
      xlabel('Valeur du poids');

      subplot(1,2,2);
      histogram(W_opt(:), 50);
      title('Apres optimisation L1');
      xlabel('Valeur du poids');
      ```
  - **Colonne droite (Métriques de sparsité)** :
    - Tableau comparant les métriques avant et après optimisation :
      - Poids = 0 : 0% (initial) vs 23% (L1).
      - \( |w| < 0.01 \) : 5% vs 45%.
      - \( |w| < 0.1 \) : 42% vs 78%.
  - **Conclusion** : La régularisation L1 réduit significativement la complexité du modèle.
- **Objectif** : Montre l'effet de la régularisation L1 sur la sparsité.

#### Diapositive : Matrice de Confusion
- **Contenu** :
  - **Diagramme** : Une matrice de confusion TikZ (10×10 pour MNIST) avec :
    - Valeurs diagonales à 95 (précision élevée pour chaque classe).
    - Dégradé de couleurs pour les erreurs hors diagonale.
    - Étiquettes pour "Prédiction" et "Vérité".
  - **Accuracy globale** : 94,2 %.
- **Objectif** : Visualise les performances de classification.

#### Diapositive : Comparaison des Méthodes
- **Contenu** :
  - **Tableau** :
    | Méthode | Accuracy | Sparsité | Temps (s) | Params |
    |---------|----------|----------|-----------|--------|
    | SGD classique | 95,1 % | 0 % | 45 | 109k |
    | Adam | 95,8 % | 0 % | 38 | 109k |
    | UNLocBox (L1) | 94,2 % | 23 % | 52 | 84k |
    | UNLocBox (L2) | 95,3 % | 0 % | 48 | 109k |
  - **Avantages d'UNLocBox avec L1** :
    - Réduction de 23 % des paramètres actifs.
    - Meilleure généralisation (moins d'overfitting).
    - Légère perte de précision compensée par la robustesse.
- **Objectif** : Compare UNLocBox avec d'autres méthodes d'optimisation.

#### Diapositive : Visualisation des Poids Appris
- **Contenu** :
  - **Diagramme** : Visualisation TikZ de trois filtres 5×5 (simulant les poids de la première couche).
  - **Observations** :
    - Détection de contours et formes simples.
    - Certains filtres désactivés par L1 (sparsité).
    - Représentations compactes et interprétables.
- **Objectif** : Montre les caractéristiques apprises par la première couche.

---

### Section 7 : Conclusion et Extensions

#### Diapositive : Résumé des Contributions
- **Contenu** :
  - **Accomplissements** :
    1. Implémentation d'un réseau de neurones avec UNLocBox.
    2. Optimisation avec régularisation L1 pour la sparsité.
    3. Comparaison avec les méthodes classiques.
    4. Visualisation complète des résultats.
  - **Résultats clés** :
    - Accuracy : 94,2 % (compétitif).
    - Sparsité : 23 % des poids éliminés.
    - Convergence stable et prévisible.
    - Modèle plus interprétable.
- **Objectif** : Résume les contributions principales du tutoriel.

#### Diapositive : Extensions Possibles
- **Contenu** :
  - **Colonne gauche** :
    - **Régularisations avancées** : Total Variation, Group Lasso, Structured Sparsity, Nuclear norm.
    - **Architectures** : CNN, RNN, Autoencoders, GANs.
  - **Colonne droite** :
    - **Optimisations** : Mini-batch, pas adaptatif, variantes de momentum, méthodes du second ordre.
    - **Applications** : Transfer learning, recherche d'architecture neuronale, pruning automatique, quantization.
- **Objectif** : Propose des directions pour étendre le travail.

#### Diapositive : Ressources et Références
- **Contenu** :
  - **Documentation** :
    - UNLocBox : URL officielle.
    - MATLAB Deep Learning : URL MathWorks.
  - **Références académiques** :
    - Perraudin et al. (2014). UNLocBoX: A MATLAB convex optimization toolbox.
    - Beck & Teboulle (2009). FISTA algorithm.
    - LeCun et al. (2015). Deep Learning (Nature).
- **Objectif** : Fournit des ressources pour approfondir le sujet.

#### Diapositive : Code Complet - Partie 1
- **Contenu** :
  - **Code** : Script principal pour l'optimisation :
    ```matlab
    %% Script Principal: Optimisation NN avec UNLocBox
    clear; clc; close all;

    % 1. Chargement et preparation des donnees
    load('mnist_data.mat');
    X_train = double(X_train) / 255;
    Y_train = ind2vec(y_train')';
    X_test = double(X_test) / 255;
    Y_test = ind2vec(y_test')';

    % 2. Architecture du reseau
    layer_sizes = [784, 128, 64, 10];
    [W, b] = initialize_network(layer_sizes);

    % 3. Parametres d'optimisation
    lambda1 = 0.001; % L1
    lambda2 = 0.01;  % L2

    % 4. Definition des fonctions UNLocBox
    f1.eval = @(theta) loss_L2(theta, X_train, Y_train, layer_sizes, lambda2);
    f1.grad = @(theta) gradient_L2(theta, X_train, Y_train, layer_sizes, lambda2);
    f2.eval = @(theta) lambda1 * norm(theta(:), 1);
    f2.prox = @(theta, gamma) soft_threshold(theta, gamma * lambda1);
    ```
- **Objectif** : Présente la première partie du script principal.

#### Diapositive : Code Complet - Partie 2
- **Contenu** :
  - **Code** : Suite du script principal :
    ```matlab
    % 5. Configuration et optimisation
    param.verbose = 1;
    param.maxit = 100;
    param.tol = 1e-4;
    param.gamma = 0.01;

    theta_init = pack_params(W, b);
    [theta_opt, info] = forward_backward(theta_init, f1, f2, param);

    % 6. Extraction et evaluation
    [W_opt, b_opt] = unpack_params(theta_opt, layer_sizes);
    [y_pred, ~] = forward_pass(X_test, W_opt, b_opt);
    [~, pred_labels] = max(y_pred, [], 1);
    [~, true_labels] = max(Y_test, [], 1);
    accuracy = mean(pred_labels == true_labels);

    fprintf('Accuracy finale: %.2f%%\n', accuracy * 100);

    % 7. Analyse de la sparsite
    sparsity = sum(abs(theta_opt) < 1e-6) / length(theta_opt);
    fprintf('Pourcentage de poids nuls: %.2f%%\n', sparsity * 100);
    ```
- **Objectif** : Montre l'optimisation, l'évaluation et l'analyse de la sparsité.

#### Diapositive : Démonstration Interactive
- **Contenu** :
  - Invite à une démonstration en direct avec le code MATLAB.
  - Visualisation TikZ d'un cercle avec le texte "Code MATLAB Disponible".
- **Objectif** : Engage l'audience pour une démonstration interactive.

#### Diapositive : Remerciements
- **Contenu** :
  - Message de remerciement : "Merci pour votre attention !"
  - Invitation aux questions.
  - Contact : `votre.email@institution.edu`.
- **Objectif** : Conclut la présentation et ouvre la discussion.

---

### Annexes

#### Annexe A : Fonctions Auxiliaires
- **Contenu** :
  - **Code** :
    ```matlab
    function [W, b] = initialize_network(layer_sizes)
        n_layers = length(layer_sizes) - 1;
        for i = 1:n_layers
            n_in = layer_sizes(i);
            n_out = layer_sizes(i+1);
            limit = sqrt(6 / (n_in + n_out));
            W{i} = 2 * limit * rand(n_out, n_in) - limit;
            b{i} = zeros(n_out, 1);
        end
    end

    function theta = pack_params(W, b)
        theta = [];
        for i = 1:length(W)
            theta = [theta; W{i}(:); b{i}];
        end
    end

    function [W, b] = unpack_params(theta, layer_sizes)
        idx = 1;
        for i = 1:length(layer_sizes)-1
            n_in = layer_sizes(i);
            n_out = layer_sizes(i+1);
            W{i} = reshape(theta(idx:idx+n_in*n_out-1), n_out, n_in);
            idx = idx + n_in*n_out;
            b{i} = theta(idx:idx+n_out-1);
            idx = idx + n_out;
        end
    end
    ```
- **Objectif** : Fournit les fonctions pour initialiser et manipuler les paramètres du réseau.

#### Annexe B : Calcul de la Perte et du Gradient
- **Contenu** :
  - **Code** :
    ```matlab
    function loss = loss_L2(theta, X, Y, layer_sizes, lambda2)
        [W, b] = unpack_params(theta, layer_sizes);
        [y_pred, ~] = forward_pass(X, W, b);
        
        % Cross-entropy loss
        N = size(Y, 2);
        ce_loss = -sum(sum(Y .* log(y_pred + 1e-8))) / N;
        
        % L2 regularization
        l2_reg = 0;
        for i = 1:length(W)
            l2_reg = l2_reg + sum(W{i}(:).^2);
        end
        
        loss = ce_loss + (lambda2/2) * l2_reg;
    end

    function grad = gradient_L2(theta, X, Y, layer_sizes, lambda2)
        [W, b] = unpack_params(theta, layer_sizes);
        [y_pred, cache] = forward_pass(X, W, b);
        [grad_W, grad_b] = backward_pass(Y, cache, W);
        
        % Add L2 regularization gradient
        for i = 1:length(W)
            grad_W{i} = grad_W{i} + lambda2 * W{i};
        end
        
        grad = pack_params(grad_W, grad_b);
    end
    ```
- **Objectif** : Fournit les fonctions pour calculer la perte et le gradient avec régularisation L2.

---

### Résumé Général
Le tutoriel est une présentation complète et pédagogique sur l'utilisation d'UNLocBox pour optimiser un réseau de neurones appliqué au jeu de données MNIST. Il couvre :
- **Introduction** : Contexte des réseaux de neurones et défis d'optimisation.
- **Configuration** : Installation d'UNLocBox et préparation des données.
- **Architecture** : Conception d'un réseau avec 784 entrées, deux couches cachées (128 et 64 neurones), et 10 sorties.
- **Formulation** : Définition de la fonction de coût (entropie croisée) et des régularisations L1/L2.
- **Optimisation** : Utilisation de l'algorithme FISTA avec UNLocBox pour minimiser la fonction objectif.
- **Visualisation** : Courbes de convergence, histogrammes de poids, matrice de confusion, et comparaison avec SGD et Adam.
- **Conclusion** : Résultats (94,2 % d'accuracy, 23 % de sparsité) et extensions possibles (CNN, RNN, etc.).
- **Annexes** : Fonctions auxiliaires pour l'initialisation, la perte et le gradient.

Le tutoriel est bien structuré, avec des visualisations claires (diagrammes TikZ) et des extraits de code MATLAB détaillés, rendant le contenu accessible à un public technique intéressé par l'optimisation des réseaux de neurones.

Si vous avez besoin d'une version PDF de ce tutoriel ou d'une analyse plus spécifique d'une section, veuillez le préciser !
